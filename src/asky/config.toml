# asky Configuration File
# This file defines general settings, API endpoints, user shortcuts, and model configurations.

# --- General Settings ---
[general]
# Name of the environment variable that stores the path to the SQLite history database.
# Default if not set: SEARXNG_HISTORY_DB_PATH
db_path_env_var = "ASKY_DB_PATH"

# Logging Configuration
# Level: DEBUG, INFO, WARNING, ERROR, CRITICAL
log_level = "INFO"
# Log file path. Defaults to ~/.config/asky/asky.log
log_file = "~/.config/asky/asky.log"

# Maximum length of the query and answer summaries shown in 'asky -H'.
query_summary_max_chars = 40
answer_summary_max_chars = 200

# Threshold for using full query vs summary in --continue-chat mode.
# If query length is below this, it's used as is.
continue_query_threshold = 160

# URL of your SearXNG instance.
searxng_url = "http://localhost:8888"

# Search provider to be used: "searxng" or "serper"
search_provider = "searxng"

# URL of the Serper API.
serper_api_url = "https://google.serper.dev/search"

# Name of the environment variable that stores the Serper API key.
serper_api_key_env = "SERPER_API_KEY"

# Maximum number of turns (tool calls) allowed in a single conversation loop.
max_turns = 20

# Timeout for API requests in seconds
request_timeout = 60

# Default context size (if not specified in the model configuration)
default_context_size = 4096

# Default model used when no model is specified via CLI (-m).
default_model = "gf"

# Model used specifically for internal text summarization tasks.
summarization_model = "lfm"

# User-Agent string to be used for search and content retrieval requests.
user_agent = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"

# Specific User-Agent string for LLM API requests.
# Some providers only accept specific user agents for certain subscriptions types.
llm_user_agent = "asky/1.0.0"

# --- Limits & Timeouts ---
[limits]
# Maximum number of links returned from get_url_details to prevent context overflow.
max_url_detail_links = 50

# Maximum snippet length in search results.
search_snippet_max_chars = 400

# Maximum recursion depth for expanding slash commands.
query_expansion_max_depth = 5

# Maximum file size for custom prompts read from file (in bytes).
max_prompt_file_size = 10240

# LLM API retry settings
max_retries = 10
initial_backoff = 2
max_backoff = 60

# Search and fetch timeouts in seconds
search_timeout = 20
fetch_timeout = 20

# --- API Definitions ---
# Define reusable API endpoints and authentication details here.
# For each section [api.NAME]:
#   url: The base URL for the chat completions endpoint.
#   api_key_env: (Recommended) Name of the environment variable containing the API key.
#   api_key: (Optional) The API key directly. Takes precedence over api_key_env if both are missing.

[api.gemini]
url = "https://generativelanguage.googleapis.com/v1beta/chat/completions"
api_key_env = "GOOGLE_API_KEY"


[api.anthropic]
url = "https://api.anthropic.com/v1/messages"
api_key_env = "ANTHROPIC_API_KEY"


[api.openai]
url = "https://api.openai.com/v1/chat/completions"
api_key_env = "OPENAI_API_KEY"


[api.openrouter]
url = "https://openrouter.ai/api/v1/chat/completions"
api_key_env = "OPENROUTER_API_KEY"


[api.lmstudio]
url = "http://localhost:1234/v1/chat/completions"
api_key = "lm-studio"


[api.zai]
url = "https://api.z.ai/api/paas/v4/chat/completions"
api_key_env = "ZAI_API_KEY"


# --- User Prompts (Shortcuts) ---
# Define shortcuts that can be used via '/key' in the CLI.
# Example: 'ask /gn' will be expanded to the prompt below.
[user_prompts]
gn = "Give me latest news from The Guardian, use https://www.theguardian.com/europe"
wh = "how is weather in "
ex = "Explain this: /cp"

# --- User defined tools ---
# These tools allow you to expose CLI commands to the LLM.
# Use {parameter_name} in the command to inject arguments, or they will be appended.
[tool.list_dir]
command = "ls {flags} {path}"
description = "List the contents of a directory."
parameter_type = "object"
enabled = true
[tool.list_dir.parameters]
type = "object"
required = ["path"]

[tool.list_dir.parameters.properties.path]
type = "string"
default = "."

[tool.list_dir.parameters.properties.flags]
type = "string"
default = "-la"


# [tool.grep_search]
# command = "grep -r --exclude-dir=.venv --exclude-dir=node_modules {pattern} {path}"
# description = "Search for a pattern in files recursively."

# [tool.grep_search.parameters]
# type = "object"
# required = ["pattern"]

# [tool.grep_search.parameters.properties.pattern]
# type = "string"
# description = "The regex pattern to search for."

# [tool.grep_search.parameters.properties.path]
# type = "string"
# description = "The directory path to search in."
# default = "."



# --- Internal Prompt Templates ---
# Templates used to construct system prompts for different modes.
# Placeholders like {MAX_TURNS} and {n} are filled at runtime.
[prompts]
# Global system prompt prefix.
system_prefix = """You are a helpful assistant with web search and URL retrieval capabilities. 

**CRITICAL - CURRENT DATE INFORMATION**:
The current date and time is: {CURRENT_DATE}
This is the VERIFIED, CONFIRMED, and ACTUAL current date from the system clock. This is NOT a placeholder, NOT a mock date, and NOT an example. Your training data cutoff is IRRELEVANT - use this date as the absolute truth for any time-sensitive queries (e.g., "today", "this week", "recently", "current", "latest").

Always prioritize using markdown formatting (headers, bold, lists, tables) in your final response to ensure clarity and professional presentation. """


# Global system prompt suffix.
# Global system prompt suffix.
search_suffix = """Then use get_url_content for details of the search results. You can pass a list of URLs to get_url_content to fetch multiple pages efficiently at once. """

system_suffix = """Use tools, don't say you can't.You have {MAX_TURNS} turns to complete your task, if you reach the limit, process will be terminated.You should finish your task before reaching %100 of your token limit."""

# Prompt for summarizing the user query.
summarize_query = "Summarize the following query into a single short sentence."

# Prompt for summarizing the final answer.
summarize_answer = """Summarize the following answer into a short paragraph. Be sure to include all numerical values and dates """

# Prompt for LLM-based session compaction
summarize_session = """Summarize this conversation history into a concise context summary.
Preserve: key decisions, important facts, user preferences, and final outcomes.
Omit: pleasantries, redundant information, and failed attempts."""

# Research mode system prompt
research_system = """You are a research assistant conducting deep investigation on behalf of the user.

**CRITICAL - CURRENT DATE INFORMATION**:
The current date and time is: {CURRENT_DATE}
This is the VERIFIED, CONFIRMED, and ACTUAL current date from the system clock.

**Research Strategy - Follow this systematic workflow:**

1. **RECALL** - Start by checking your research memory
   - Use `query_research_memory` to see if you've researched this topic before
   - Past findings can provide a foundation and save time
   - Build on previous discoveries rather than starting from scratch

2. **DISCOVER** - Use `extract_links` to explore pages and discover available information
   - Start with web_search to find relevant sources
   - Use extract_links on promising URLs to see what content is available
   - Provide a query parameter to filter links by relevance to your research
   - Content is cached automatically - you only see link labels and URLs

3. **EVALUATE** - Review link labels and relevance scores
   - Prioritize high-relevance links (scores closer to 1.0)
   - Look for primary sources, research papers, authoritative sites
   - Identify which links likely have substantive information vs navigation/ads

4. **SKIM** - Use `get_link_summaries` to preview promising pages
   - Read AI-generated summaries to decide if full content is needed
   - Summaries are generated in the background - may show "processing" initially
   - This saves tokens and speeds up your research

5. **DEEP READ** - Use `get_relevant_content` for targeted extraction
   - Provide a specific query to retrieve only relevant content sections
   - Use `get_full_content` only when comprehensive understanding is needed
   - The RAG system finds semantically similar content chunks

6. **REMEMBER** - Use `save_finding` to store important discoveries
   - Save key facts, statistics, and insights as you find them
   - Include source URL and descriptive tags for easy retrieval
   - These findings persist across sessions for future research

7. **ITERATE** - Repeat as needed to build comprehensive understanding
   - Follow promising leads to new pages
   - Cross-reference information across multiple sources
   - Synthesize findings into a coherent answer

**Token Efficiency Guidelines:**
- Prefer summaries over full content when possible
- Use targeted queries with get_relevant_content to minimize token usage
- Don't request content you won't use
- Batch multiple URL requests when possible

**Available Tools:**
- `web_search` - Search the web for relevant sources
- `extract_links` - Discover links on pages (caches content, returns only links)
- `get_link_summaries` - Get AI summaries of cached pages
- `get_relevant_content` - RAG-based retrieval of relevant content sections
- `get_full_content` - Get complete cached content (use sparingly)
- `save_finding` - Save a discovered fact to research memory
- `query_research_memory` - Search your research memory for past findings

Always prioritize using markdown formatting in your final response."""

# Prompt for summarizing cached page content (used in background)
summarize_page = """Summarize this webpage content concisely in 2-3 sentences.
Highlight: main topic, key facts, and what information this page provides.
Be factual and specific - include any important numbers, dates, or names."""

# --- Session Settings ---
[session]
# Trigger compaction at this % of model context
compaction_threshold = 80

# Compaction strategy: "summary_concat" or "llm_summary"
compaction_strategy = "summary_concat"

# --- Model Definitions ---
# Each section [models.NAME] configures a specific model:
#   id: The exact model ID used by the API provider.
#   api: Reference to a name defined in the [api] section.
#   context_size: Total context window size (tokens/chars approximation) for trimming history.

# Note: max_chars is the context_size values of the following models are arbitrarily set for my own use.
# Check model provider's documentation for the actual context size of the models.
# Experiment with max_chars to find the optimal value for your use case (depending on your needs, and model/hardware capabilities)

[models.gf]
id = "gemini-flash-latest"
api = "gemini"
context_size = 1000000

[models.glmair]
id = "glm-4.5-air"
api = "zai"
context_size = 100000

[models.glmflash]
id = "glm-4.7-flash"
api = "zai"
context_size = 100000

[models.q34t]
id = "qwen/qwen3-4b-thinking-2507"
api = "lmstudio"
context_size = 32000

[models.q34]
id = "qwen/qwen3-4b-2507"
api = "lmstudio"
context_size = 32000

[models.lfm]
id = "liquid/lfm2.5-1.2b"
api = "lmstudio"
context_size = 32000

[models.q8]
id = "qwen/qwen3-8b"
api = "lmstudio"
context_size = 32000

[models.q30]
id = "qwen/qwen3-30b-a3b-2507"
api = "lmstudio"
context_size = 32000

# --- Research Mode Settings ---
# Deep research mode with RAG-based content retrieval
[research]
# Enable research mode tools
enabled = true

# Cache TTL in hours (cached pages expire after this time)
cache_ttl_hours = 24

# Maximum links to return per URL (before relevance filtering)
max_links_per_url = 50

# Maximum links after relevance filtering (when query is provided)
max_relevant_links = 20

# Chunk size for content splitting (characters)
chunk_size = 1000

# Chunk overlap for context continuity (characters)
chunk_overlap = 200

# Number of relevant chunks to retrieve per URL in RAG queries
max_chunks_per_retrieval = 5

# Background summarization thread pool size
summarization_workers = 2

# Maximum findings to return from research memory queries
memory_max_results = 10

# Embedding API settings (LM Studio compatible)
[research.embedding]
# API endpoint for embeddings (OpenAI-compatible format)
api_url = "http://localhost:1234/v1/embeddings"

# Model to use for embeddings (must be loaded in LM Studio)
model = "gaianet/text-embedding-nomic-embed-text-v1.5-embedding"

# Embedding dimension (depends on model - nomic is 768)
dimension = 768

# Batch size for embedding requests
batch_size = 32

# Timeout for embedding API requests in seconds
timeout = 30

# --- Email Settings ---
[email]
smtp_host = "smtp.gmail.com"
smtp_port = 587
# Use smtp_use_ssl = true for port 465 (direct SSL/TLS connection)
# Use smtp_use_tls = true for port 587 (STARTTLS upgrade)
smtp_use_ssl = false
smtp_use_tls = true
# Authentication - use env var (recommended) or direct value
smtp_user_env = "ASKY_SMTP_USER"
smtp_password_env = "ASKY_SMTP_PASSWORD"
# Default sender address (optional, defaults to smtp_user)
from_address = ""

# --- Push Data Configuration ---
# Define HTTP endpoints to push query results to external services.
# Each endpoint can be enabled for LLM tool use and/or CLI invocation.
#
# Field value types:
#   - Static: "literal_value" - used as-is
#   - Environment: key_env = "ENV_VAR_NAME" - read from environment variable
#   - Dynamic: "${param}" - provided by LLM tool call or CLI --push-param
#   - Special: "${query}", "${answer}", "${timestamp}", "${model}" - auto-filled
#
# Example endpoint configuration (commented out by default):
#
# [push_data.example_webhook]
# url = "https://webhook.site/your-unique-id"
# method = "post"  # "get" or "post"
# enabled = false  # Set to true to expose as LLM tool
# description = "Post findings to webhook"
#
# # Optional headers
# [push_data.example_webhook.headers]
# Content-Type = "application/json"
# Authorization_env = "MY_AUTH_TOKEN"  # Read from env var
#
# # Request payload fields
# [push_data.example_webhook.fields]
# static_field = "some_value"          # Static value
# api_key_env = "MY_API_KEY"           # From environment variable
# title = "${title}"                   # Dynamic from LLM/CLI
# content = "${answer}"                # Special variable (answer text)
# query_text = "${query}"              # Special variable (query text)
# model_used = "${model}"              # Special variable (model alias)
# timestamp = "${timestamp}"           # Special variable (ISO timestamp)

